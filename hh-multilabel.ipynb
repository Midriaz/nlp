{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автор задачи компания HeadHunter.\n",
    "\n",
    "Выбор нужных специализаций для вакансии не всегда тривиален, но при этом набор специализаций сильно влияет на то, будет ли показана вакансия нужным соискателям. Необходимо помочь работодателю быстрее сделать правильный выбор. По контенту вакансий нужно предсказать набор специализаций.\n",
    "\n",
    "Каждая вакансия на hh.ru протеггирована набором специализаций: от 1 до 6-ти штук. Специализации – категориальный признак, принимающий одно из 620-ти значений, подробнее можно посмотреть, например, в API HeadHunter. \n",
    "\n",
    "Датасет (набор данных) состоит из ~2,9 млн вакансий, которые представлены в формате json. Задача состоит в том, чтобы по контенту вакансий предсказать набор специализаций. Оценка качества будет проводиться по среднему значению f-score предсказанных наборов по каждой вакансии.\n",
    "\n",
    "Набор данных будет разделен на 2 части — обучающая выборка (train) и тестовая (test), для проверки и подсчета метрик качества решения. Данные будут разбиты случайным образом.\n",
    "\n",
    "По вакансиям будут даны следующие данные: \n",
    "\n",
    "название\n",
    "текстовое описание (может содержать HTML разметку)\n",
    "ключевые навыки\n",
    "регион размещения\n",
    "Id работодателя в виде хэша\n",
    "вилка заработной платы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:19:57.873840Z",
     "start_time": "2020-07-23T08:18:58.804267Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, BertConfig, AdamW # https://huggingface.co/transformers/\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "from functions import BertForMultiLabelSequenceClassification, read_vacancies_part\n",
    "from functions import mean_f1score, f1score, decode_labels, decode_preds, sigmoid, cleanhtml\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:19:58.172121Z",
     "start_time": "2020-07-23T08:19:57.874852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GeForce RTX 2080 Ti GPUs\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device == torch.device('cpu'):\n",
    "    print('Using cpu')\n",
    "else:\n",
    "    print('Using {} GPUs'.format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:01.838943Z",
     "start_time": "2020-07-23T08:19:58.174100Z"
    }
   },
   "outputs": [],
   "source": [
    "train_specializations = pd.read_csv('train_labels.csv.gz', compression='gzip')\n",
    "train_specializations = {\n",
    "    vacancy_id: list(map(int, specs[1:-1].split(',')))\n",
    "    for vacancy_id, specs in train_specializations.set_index('vacancy_id')['specializations'].iteritems()\n",
    "}\n",
    "labels = [spec for value in train_specializations.values() for spec in value]\n",
    "    \n",
    "ohe = {cls:i for i, cls in enumerate(set(labels))}\n",
    "ohe_count = len(set(ohe.values()))\n",
    "\n",
    "# для декодинга\n",
    "revert_ohe = {value:key for key, value in ohe.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:01.843826Z",
     "start_time": "2020-07-23T08:20:01.839853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Список классов, которые имеют меньше 100 вхождений\n",
    "exclude = [760, 761, 763, 754, 738, 741, 745, 743, 762, 570, 739, 740, 759, 735, 753, 746, 747, 554, 749,\n",
    "550, 737, 756, 757, 742, 733, 736, 748, 744, 755, 750, 758, 200, 752, 159, 734, 394, 421, 582, 751]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:06:51.919880Z",
     "start_time": "2020-07-22T11:06:51.916889Z"
    }
   },
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:02.026073Z",
     "start_time": "2020-07-23T08:20:01.844824Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class MetaClassifier(torch.nn.Module):\n",
    "    def __init__(self, total_class: int, predict_class: int, bert_title: str, bert_text: str, input_features, device = None):\n",
    "        super(MetaClassifier, self).__init__()\n",
    "        t = torch.nn\n",
    "        self.bert_title = BertModel.from_pretrained(bert_title)\n",
    "        self.bert_title_path = bert_title\n",
    "        self.bert_text = BertModel.from_pretrained(bert_text)\n",
    "        self.bert_text_path = bert_text\n",
    "        self.features_fc = t.Sequential(t.Linear(input_features, total_class), t.ReLU(), t.Dropout(p=0.3))\n",
    "        self.berts_fc = t.Sequential(t.Linear(768*2, total_class), t.ReLU(), t.Dropout(p=0.1))\n",
    "        self.stack_fc = t.Sequential(t.Linear(total_class*2, total_class), t.ReLU())\n",
    "        self.dp = t.Dropout(p=0.1)\n",
    "        self.output = t.Linear(total_class, predict_class)\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "            self.to(self.device)\n",
    "        \n",
    "    def forward(self, x, title, text):\n",
    "        _, out_title = self.bert_title(title['x'], None, title['attention'])\n",
    "        _, out_text = self.bert_text(text['x'], None, text['attention'])\n",
    "\n",
    "        out_berts = self.berts_fc(torch.cat([out_title, out_text], dim=1))        \n",
    "        del out_text, out_title       \n",
    "        out_features = self.features_fc(x)\n",
    "        \n",
    "        out = self.stack_fc(torch.cat([out_features, out_berts], dim=1))\n",
    "        del out_features, out_berts\n",
    "        out = self.dp(out)\n",
    "        out = self.output(out) \n",
    "        return out                           \n",
    "    \n",
    "    def save_berts(self):\n",
    "        self.bert_title.save_pretrained(self.bert_title_path)\n",
    "        self.bert_text.save_pretrained(self.bert_text_path)\n",
    "        return True\n",
    "    \n",
    "    def change_save_paths(self, bert_title = '', bert_text = ''):\n",
    "        if bert_title != '':\n",
    "            self.bert_title_path = bert_title\n",
    "        if bert_text != '':\n",
    "            self.bert_text_path = bert_text\n",
    "            \n",
    "    def get_outputs(self, loader, train=True, validate=False):\n",
    "        \"\"\"\n",
    "        Return output of the model for loader\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            pass\n",
    "        else:\n",
    "            valid_preds = []\n",
    "            if validate:\n",
    "                valid_labels = []\n",
    "            else:\n",
    "                # Если не обучаем и не валидация - выдаем вместо лейблов номера вакансий\n",
    "                vacancy = []\n",
    "                \n",
    "        for step, batch in enumerate(loader):\n",
    "            if step % 1000 == 0:\n",
    "                now = datetime.datetime.now()\n",
    "                print('{} Осталось {} из {}'.format(now.strftime(\"%H:%M:%S\"), step, len(loader)))\n",
    "            bert_title, bert_text = {}, {}\n",
    "            \n",
    "            # Передавать метку или нет\n",
    "            features, bert_title['x'], bert_title['attention'], bert_text_x, bert_text_at, labels = batch\n",
    "            if train:\n",
    "                labels = labels.to(self.device)\n",
    "            \n",
    "            features = features.type(torch.FloatTensor)       \n",
    "            features = features.to(self.device)\n",
    "            bert_title['x'] = bert_title['x'].type(torch.LongTensor)\n",
    "            bert_title['x'] = bert_title['x'].to(self.device)\n",
    "            bert_title['attention'] = bert_title['attention'].type(torch.LongTensor)\n",
    "            bert_title['attention'] = bert_title['attention'].to(self.device)\n",
    "            \n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Длина текстов часто превышает 512 токенов, поэтому я делаю отдельный расчет по частям и потом усредняю выход\n",
    "            for i in range(bert_text_x.shape[2]):\n",
    "                tokens = bert_text_x[:, :, i]\n",
    "                if step % 1000 == 0:\n",
    "                    print(tokens[0])\n",
    "                \n",
    "                if tokens.sum() == 0:\n",
    "                    pass\n",
    "                tokens = tokens.type(torch.LongTensor)\n",
    "                tokens = tokens.to(device)\n",
    "                attention = bert_text_at[:, :, i]\n",
    "                attention = attention.type(torch.LongTensor).to(device)\n",
    "                \n",
    "                # вклад текущего текста (более короткие тексты меньше приносят вклад)\n",
    "                token_count = 1 #len(tokens)/512\n",
    "                #for token in tokens:\n",
    "                #    if token > 0:\n",
    "                #        token_count += 1\n",
    "                #token_count = len(tokens)/token_count \n",
    "                \n",
    "                if i == 0:\n",
    "                    if train:\n",
    "                        output = model(features, bert_title, {'x': tokens, 'x_id': None, 'attention': attention})\n",
    "                    else:\n",
    "                        with torch.no_grad():\n",
    "                            output = model(features, bert_title, {'x': tokens, 'x_id': None, 'attention': attention})\n",
    "                    predict = output\n",
    "                    \n",
    "                else:\n",
    "                    # работает очень медленно, если обучать каждый раз\n",
    "                    with torch.no_grad():\n",
    "                        output = model(features, bert_title, {'x': tokens, 'x_id': None, 'attention': attention})\n",
    "                    predict = torch.add(predict, output, alpha=token_count)\n",
    "            output = torch.div(predict, bert_text_x.shape[2])\n",
    "            \n",
    "            if train:\n",
    "                # Backward pass\n",
    "                loss = floss(output, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                # Обновляем параметры и делаем шаг используя посчитанные градиенты\n",
    "                scheduler.step(loss)\n",
    "                optimizer.step()\n",
    "\n",
    "                # Обновляем loss\n",
    "                train_loss_set.append(float(loss))  \n",
    "                train_loss += float(loss)\n",
    "                \n",
    "                # Рисуем график\n",
    "                if step % 1000 == 0:\n",
    "                    clear_output(True)\n",
    "                    plt.plot(train_loss_set)\n",
    "                    plt.title(\"Training loss part={} EPOCH={} learning rage = {}\".format(part_num, e, optimizer.param_groups[0]['lr']))\n",
    "                    plt.xlabel(\"Batch\")\n",
    "                    plt.ylabel(\"Loss\")\n",
    "                    plt.ylim(0, 0.03)\n",
    "                    plt.show()\n",
    "            else:\n",
    "                output = output.detach().cpu().numpy()\n",
    "                valid_preds.extend(output)\n",
    "                labels = labels.to('cpu').numpy()\n",
    "                if validate:\n",
    "                    valid_labels.extend(labels)\n",
    "                else:\n",
    "                    vacancy.extend(labels)\n",
    "        if train:\n",
    "            return train_loss, train_loss_set\n",
    "        elif validate:\n",
    "            return valid_preds, valid_labels\n",
    "        else:\n",
    "            return valid_preds, vacancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чтение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кодировщики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:02.349406Z",
     "start_time": "2020-07-23T08:20:02.028014Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def calculate_money(cfrom, cto):\n",
    "    median_value = 27.5 #Подсчитано в explore - нули не считаются, верхний лимит для одной зп 10 млн\n",
    "    cap = 10000\n",
    "    mean_money = []\n",
    "    for key in range(len(cfrom)):\n",
    "        if cfrom[key] is None and cto[key] is not None:\n",
    "            mean_money.append(cto[key])\n",
    "        elif cfrom[key] is not None and cto[key] is None:\n",
    "            mean_money.append(cfrom[key])\n",
    "        elif cfrom[key] is None and cto[key] is None:\n",
    "            mean_money.append(median_value)\n",
    "        else:\n",
    "            mean_money.append((cfrom[key] + cto[key]) / 2000)\n",
    "        \n",
    "        if mean_money[-1] > cap:\n",
    "            mean_money[-1] = cap\n",
    "    return mean_money\n",
    "\n",
    "def currency_none(currency):\n",
    "    for i in range(len(currency)):\n",
    "        if currency[i] is None:\n",
    "            currency[i] = 'XXX'\n",
    "    return currency\n",
    "\n",
    "def skill_tokenize(skill):\n",
    "    global tokenizer\n",
    "    result = []\n",
    "    for s in skill:\n",
    "        ts = tokenizer.tokenize(s)\n",
    "        ts.insert(0, '[CLS] ')\n",
    "        ts.append(\" [SEP]\")\n",
    "        result.extend(ts)\n",
    "    return result\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('pretrained/', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ce8f7244ee4e9da27510b3a741c5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employment, schedule, experience, currency, employer, cto, cfrom = [], [], [], [], [], [], []\n",
    "\n",
    "for part_num in tqdm(range(1,11)):\n",
    "    part = read_vacancies_part(part_num)\n",
    "    employment.extend(vacancy['employment'] for vacancy in part.values())\n",
    "    schedule.extend(vacancy['work_schedule'] for vacancy in part.values())\n",
    "    currency.extend(vacancy['currency'] for vacancy in part.values())\n",
    "    experience.extend(vacancy['work_experience'] for vacancy in part.values())\n",
    "    employer.extend(vacancy['employer'] for vacancy in part.values())\n",
    "    cto.extend(vacancy['compensation_from'] for vacancy in part.values())\n",
    "    cfrom.extend(vacancy['compensation_to'] for vacancy in part.values())\n",
    "del part\n",
    "\n",
    "money = calculate_money(cfrom, cto)\n",
    "\n",
    "encoders = {\n",
    "    'employment': OneHotEncoder(sparse=False).fit(np.array(employment).reshape(-1, 1)),\n",
    "    'schedule': OneHotEncoder(sparse=False).fit(np.array(schedule).reshape(-1, 1)),\n",
    "    'currency': OneHotEncoder(sparse=False).fit(np.array(currency_none(currency)).reshape(-1, 1)),\n",
    "    'experience': OneHotEncoder(sparse=False).fit(np.array(experience).reshape(-1, 1)),\n",
    "    'employer': LabelEncoder().fit(employer),\n",
    "    'y': ohe,\n",
    "    'y_decode': revert_ohe,\n",
    "    'money': MinMaxScaler().fit(np.array(money).reshape(-1, 1))\n",
    "}\n",
    "del employment, schedule, currency, experience, employer, money\n",
    "\n",
    "f = open('encoders.pkl', 'wb')\n",
    "pickle.dump(encoders, f)\n",
    "f.close()\n",
    "\n",
    "del encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills, regions, employment, schedule, experience, currency, employer, cfrom, cto, date = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "text, names = {}, {}\n",
    "\n",
    "for part_num in range(1,11):\n",
    "    print(f\"Начинаем {part_num}\")\n",
    "    part = read_vacancies_part(part_num)\n",
    "    skills.update({int(vacancy_id): vacancy['key_skills'] for vacancy_id, vacancy in part.items()})\n",
    "    text.update({int(vacancy_id): vacancy['description'] for vacancy_id, vacancy in part.items()})\n",
    "    names.update({int(vacancy_id): vacancy['name'] for vacancy_id, vacancy in part.items()})\n",
    "    regions.update({int(vacancy_id): vacancy['area_id'] for vacancy_id, vacancy in part.items()})\n",
    "    employment.update({int(vacancy_id): vacancy['employment'] for vacancy_id, vacancy in part.items()})\n",
    "    schedule.update({int(vacancy_id): vacancy['work_schedule'] for vacancy_id, vacancy in part.items()})\n",
    "    currency.update({int(vacancy_id): vacancy['currency'] for vacancy_id, vacancy in part.items()})\n",
    "    experience.update({int(vacancy_id): vacancy['work_experience'] for vacancy_id, vacancy in part.items()})\n",
    "    employer.update({int(vacancy_id): vacancy['employer'] for vacancy_id, vacancy in part.items()})\n",
    "    date.update({int(vacancy_id): vacancy['creation_date'] for vacancy_id, vacancy in part.items()})\n",
    "    cfrom.update({int(vacancy_id): vacancy['compensation_from'] for vacancy_id, vacancy in part.items()})\n",
    "    cto.update({int(vacancy_id): vacancy['compensation_to'] for vacancy_id, vacancy in part.items()})\n",
    "    \n",
    "    # Готовим данные\n",
    "    X_names, X_skills, X_text, X_regions, X_employment, X_schedule, X_currency, X_experience  = [], [], [], [], [], [], [], []\n",
    "    X_employer, X_date, X_cfrom, X_cto, y = [], [], [], [], []\n",
    "    Xs_names, Xs_skills, Xs_text, Xs_regions, Xs_employment, Xs_schedule, Xs_currency, Xs_experience  = [], [], [], [], [], [], [], []\n",
    "    Xs_employer, Xs_date, Xs_cfrom, Xs_cto = [], [], [], []\n",
    "    for vacancy in names.keys():\n",
    "        # размечанные данные\n",
    "        if vacancy in train_specializations:\n",
    "            X_names.append(names[vacancy])\n",
    "            X_skills.append(skills[vacancy])\n",
    "            X_text.append(text[vacancy])\n",
    "            X_regions.append(regions[vacancy])\n",
    "            X_employment.append(employment[vacancy])\n",
    "            X_schedule.append(schedule[vacancy])\n",
    "            X_currency.append(currency[vacancy])\n",
    "            X_experience.append(experience[vacancy])\n",
    "            X_employer.append(employer[vacancy])\n",
    "            X_date.append(date[vacancy])\n",
    "            X_cfrom.append(cfrom[vacancy])\n",
    "            X_cto.append(cto[vacancy])\n",
    "            y.append(train_specializations[vacancy])\n",
    "        # неразмечанные данные\n",
    "        else:\n",
    "            pass # закоментировать на первом прогоне\n",
    "            Xs_names.append(names[vacancy])\n",
    "            Xs_skills.append(skills[vacancy])\n",
    "            Xs_text.append(text[vacancy])\n",
    "            Xs_regions.append(regions[vacancy])\n",
    "            Xs_employment.append(employment[vacancy])\n",
    "            Xs_schedule.append(schedule[vacancy])\n",
    "            Xs_currency.append(currency[vacancy])\n",
    "            Xs_experience.append(experience[vacancy])\n",
    "            Xs_employer.append(employer[vacancy])\n",
    "            Xs_date.append(date[vacancy])\n",
    "            Xs_cfrom.append(cfrom[vacancy])\n",
    "            Xs_cto.append(cto[vacancy])\n",
    "                \n",
    "    skills, regions, employment, schedule, experience, currency, employer, cfrom, cto, date = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    text, names = {}, {}\n",
    "        \n",
    "    print('БЕРТ по заголовку')\n",
    "    X_names = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in X_names]\n",
    "    X_names = [tokenizer.tokenize(sent) for sent in X_names]\n",
    "    print('Заголовки токенизированы')\n",
    "    print('БЕРТ по скиллам')\n",
    "    X_skills = [skill_tokenize(sent) for sent in X_skills]\n",
    "    print('Скиллы токенизированы')\n",
    "    \n",
    "    for i in range(len(X_names)):\n",
    "        X_names[i].extend(X_skills[i])\n",
    "    del X_skills\n",
    "    \n",
    "    print('БЕРТ по тексту')\n",
    "    X_text = [\"[CLS] \" + cleanhtml(x) + \" [SEP]\" for x in X_text]\n",
    "    X_text = [tokenizer.tokenize(sent) for sent in tqdm(X_text)]\n",
    "    \n",
    "    f = open(f'train{part_num}.pkl', 'wb')\n",
    "    pickle.dump(dict(\n",
    "        title = X_names,\n",
    "        text = X_text,\n",
    "        regions = X_regions,\n",
    "        employment = X_employment,\n",
    "        schedule = X_schedule,\n",
    "        currency = X_currency,\n",
    "        employer = X_employer,\n",
    "        date = X_date,\n",
    "        cfrom = X_cfrom,\n",
    "        cto = X_cto,\n",
    "        y = y,\n",
    "        ohe = ohe,\n",
    "        revert_ohe = revert_ohe,\n",
    "    ), f)\n",
    "    f.close()\n",
    "    \n",
    "del X_names, X_regions, X_employment, X_schedule, X_currency, X_experience, X_employer, X_date, X_cfrom, X_cto, y\n",
    "del Xs_names, Xs_skills, Xs_text, Xs_regions, Xs_employment, Xs_schedule, Xs_currency, Xs_experience, Xs_employer, Xs_date, Xs_cfrom, Xs_cto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка и кодировка данных для самбита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T10:52:07.526942Z",
     "start_time": "2020-07-22T10:49:22.812476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем 1\n",
      "Начинаем 2\n",
      "Начинаем 3\n",
      "Начинаем 4\n",
      "Начинаем 5\n",
      "Начинаем 6\n",
      "Начинаем 7\n",
      "Начинаем 8\n",
      "Начинаем 9\n",
      "Начинаем 10\n"
     ]
    }
   ],
   "source": [
    "skills, regions, employment, schedule, experience, currency, employer, cfrom, cto, date = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "text, names = {}, {}\n",
    "\n",
    "for part_num in range(1,11):\n",
    "    print(f\"Начинаем {part_num}\")\n",
    "    part = read_vacancies_part(part_num)\n",
    "    skills.update({int(vacancy_id): vacancy['key_skills'] for vacancy_id, vacancy in part.items()})\n",
    "    text.update({int(vacancy_id): vacancy['description'] for vacancy_id, vacancy in part.items()})\n",
    "    names.update({int(vacancy_id): vacancy['name'] for vacancy_id, vacancy in part.items()})\n",
    "    regions.update({int(vacancy_id): vacancy['area_id'] for vacancy_id, vacancy in part.items()})\n",
    "    employment.update({int(vacancy_id): vacancy['employment'] for vacancy_id, vacancy in part.items()})\n",
    "    schedule.update({int(vacancy_id): vacancy['work_schedule'] for vacancy_id, vacancy in part.items()})\n",
    "    currency.update({int(vacancy_id): vacancy['currency'] for vacancy_id, vacancy in part.items()})\n",
    "    experience.update({int(vacancy_id): vacancy['work_experience'] for vacancy_id, vacancy in part.items()})\n",
    "    employer.update({int(vacancy_id): vacancy['employer'] for vacancy_id, vacancy in part.items()})\n",
    "    date.update({int(vacancy_id): vacancy['creation_date'] for vacancy_id, vacancy in part.items()})\n",
    "    cfrom.update({int(vacancy_id): vacancy['compensation_from'] for vacancy_id, vacancy in part.items()})\n",
    "    cto.update({int(vacancy_id): vacancy['compensation_to'] for vacancy_id, vacancy in part.items()})\n",
    "    \n",
    "    X_names, X_skills, X_text, X_regions, X_employment, X_schedule, X_currency, X_experience  = [], [], [], [], [], [], [], []\n",
    "    X_employer, X_date, X_cfrom, X_cto, y = [], [], [], [], []\n",
    "    Xs_names, Xs_skills, Xs_text, Xs_regions, Xs_employment, Xs_schedule, Xs_currency, Xs_experience  = [], [], [], [], [], [], [], []\n",
    "    Xs_employer, Xs_date, Xs_cfrom, Xs_cto, Xs_vacancy = [], [], [], [], []\n",
    "    for vacancy in names.keys():\n",
    "        # неразмечанные данные\n",
    "        if vacancy not in train_specializations:\n",
    "            Xs_vacancy.append(vacancy)\n",
    "            Xs_names.append(names[vacancy])\n",
    "            Xs_skills.append(skills[vacancy])\n",
    "            Xs_text.append(text[vacancy])\n",
    "            Xs_regions.append(regions[vacancy])\n",
    "            Xs_employment.append(employment[vacancy])\n",
    "            Xs_schedule.append(schedule[vacancy])\n",
    "            Xs_currency.append(currency[vacancy])\n",
    "            Xs_experience.append(experience[vacancy])\n",
    "            Xs_employer.append(employer[vacancy])\n",
    "            Xs_date.append(date[vacancy])\n",
    "            Xs_cfrom.append(cfrom[vacancy])\n",
    "            Xs_cto.append(cto[vacancy])\n",
    "    \n",
    "    skills, regions, employment, schedule, experience, currency, employer, cfrom, cto, date = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    text, names = {}, {}\n",
    "    \n",
    "    f = open(f'submit{part_num}.pkl', 'wb')\n",
    "    pickle.dump(dict(\n",
    "        vacancy = Xs_vacancy,\n",
    "        names = Xs_names,\n",
    "        skills = Xs_skills,\n",
    "        text = Xs_text,\n",
    "        regions = Xs_regions,\n",
    "        employment = Xs_employment,\n",
    "        schedule = Xs_schedule,\n",
    "        currency = Xs_currency,\n",
    "        employer = Xs_employer,\n",
    "        date = Xs_date,\n",
    "        cfrom = Xs_cfrom,\n",
    "        cto = Xs_cto\n",
    "    ), f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка фичей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка токенизированных текстов и других данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:02.354393Z",
     "start_time": "2020-07-23T08:20:02.350403Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_param_data(part_num, train=True):\n",
    "    if train:\n",
    "        path = f'train{part_num}.pkl'\n",
    "    else:\n",
    "        path = f'submit{part_num}.pkl'\n",
    "    f = open(path, 'rb')\n",
    "    params = pickle.load(f)\n",
    "    f.close()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:02.492983Z",
     "start_time": "2020-07-23T08:20:02.356388Z"
    }
   },
   "outputs": [],
   "source": [
    "# One hot encoding для y\n",
    "def encode_y(y, ohe):\n",
    "    for i in range(len(y)):\n",
    "        y_ohe = np.zeros(ohe_count)\n",
    "        for spec in y[i]:\n",
    "            y_ohe[ohe[spec]] = 1.0\n",
    "        y[i] = y_ohe\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:02.618187Z",
     "start_time": "2020-07-23T08:20:02.493884Z"
    }
   },
   "outputs": [],
   "source": [
    "def split(dataset, size):\n",
    "    #dataset = dataset.shuffle()\n",
    "    size = int(len(dataset) * size)\n",
    "    test_dataset, train_dataset = torch.utils.data.random_split(dataset, [size, len(dataset) - size])\n",
    "    del dataset\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler= RandomSampler(train_dataset),\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    del train_dataset\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler = SequentialSampler(test_dataset),\n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    del test_dataset\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:02.753986Z",
     "start_time": "2020-07-23T08:20:02.619185Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_bert_data(X, maxlen=512):\n",
    "    global tokenizer\n",
    "    input_ids = []\n",
    "    for x in X:\n",
    "        if len(x) > 0:\n",
    "            input_ids.append(tokenizer.convert_tokens_to_ids(x))\n",
    "        else:\n",
    "            input_ids.append(np.zeros((maxlen,)))\n",
    "\n",
    "    del X\n",
    "    input_ids = pad_sequences(\n",
    "        input_ids,\n",
    "        maxlen=maxlen,\n",
    "        dtype=\"long\",\n",
    "        truncating=\"post\",\n",
    "        padding=\"post\"\n",
    "    )\n",
    "    attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "    return {'x': torch.tensor(input_ids), 'x_id': None, 'attention': torch.tensor(attention_masks)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:02.887803Z",
     "start_time": "2020-07-23T08:20:02.760885Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_encoders():\n",
    "    f = open('encoders.pkl', 'rb')\n",
    "    ohe = pickle.load(f)\n",
    "    f.close()\n",
    "    return ohe\n",
    "    \n",
    "    \n",
    "def prepare_non_text_features(params):   \n",
    "    regions = np.array(params['regions']).reshape(-1, 1)\n",
    "    del params['regions']\n",
    "\n",
    "    encoders = load_encoders()\n",
    "    employment = encoders['employment'].transform(np.array(params['employment']).reshape(-1, 1))\n",
    "    del params['employment']\n",
    "\n",
    "    schedule = encoders['schedule'].transform(np.array(params['schedule']).reshape(-1, 1))\n",
    "    del params['schedule']\n",
    "    \n",
    "    currency = currency_none(params['currency'])\n",
    "    del params['currency']\n",
    "    currency = encoders['currency'].transform(np.array(currency).reshape(-1, 1))\n",
    "    # удалить последнюю валюту (которая была неизвестна)\n",
    "    currency = currency[:, :-1]\n",
    "\n",
    "    employer = encoders['employer'].transform(params['employer'])\n",
    "    del params['employer']\n",
    "    employer = list(employer)\n",
    "    for i in range(len(employer)):\n",
    "        employer[i] = [employer[i]]\n",
    "\n",
    "    date = []\n",
    "    for d in params['date']:\n",
    "        date.append([int(d[5:7])])\n",
    "    del params['date']\n",
    "\n",
    "    money = calculate_money(params['cfrom'], params['cto'])\n",
    "    del params['cfrom'], params['cto']\n",
    "    money = encoders['money'].transform(np.array(money).reshape(-1, 1))\n",
    "\n",
    "    return np.hstack((regions, employment, schedule, currency, employer, date, money))\n",
    "\n",
    "\n",
    "def prepare_text_feature(text):\n",
    "    # текст может не помещаться (макс количество токенов - 2982)\n",
    "    # мы берем среднее по всем проходам нейронки с разными частями  \n",
    "    stack_dim_x, stack_dim_xid, stack_dim_attention = [], [], []\n",
    "    run = True\n",
    "    # медленное решение\n",
    "    #while run:\n",
    "    run = False\n",
    "    bert_text_x = []\n",
    "    for i in range(len(text)):\n",
    "        # флаг остались у нас еще длинные тексты\n",
    "        if len(text[i]) > 0:\n",
    "            run = True\n",
    "            bert_text_x.append(text[i][:512]) #стак по номеру эксперимента\n",
    "            text[i] = text[i][512:]\n",
    "        else:\n",
    "            bert_text_x.append([])\n",
    "    if run:\n",
    "        bert_text_x = prepare_bert_data(bert_text_x)\n",
    "        # stack_dim_xid.append(bert_text_x['x_id'])\n",
    "        stack_dim_attention.append(bert_text_x['attention'])\n",
    "        stack_dim_x.append(bert_text_x['x'])\n",
    "    # стакаем несколько входов для текстов\n",
    "    stack_dim_x = torch.stack(tuple(stack_dim_x), dim=2)\n",
    "    stack_dim_attention = torch.stack(tuple(stack_dim_attention), dim=2)\n",
    "    return stack_dim_x, None,stack_dim_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:20:22.600210Z",
     "start_time": "2020-07-23T08:20:02.890793Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = 'metamodel.bin'\n",
    "PATH_BERT_TITLE = 'pretrained/'\n",
    "PATH_BERT_TEXT = 'pretrained/'\n",
    "\n",
    "model = MetaClassifier(620, 620, PATH_BERT_TITLE, PATH_BERT_TEXT, 24, device=device)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "floss = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "Перед началом обучения была использована публично доступная предобученная модель RuBert: http://docs.deeppavlov.ai/en/master/features/pretrained_vectors.html#bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T14:50:50.159282Z",
     "start_time": "2020-07-22T14:50:49.880030Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:21:57.037892Z",
     "start_time": "2020-07-23T08:21:57.032904Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "EPOCHS = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10000, factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T12:11:54.447044Z",
     "start_time": "2020-07-23T10:45:50.369501Z"
    }
   },
   "outputs": [],
   "source": [
    "# pipeline\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "\n",
    "model.change_save_paths(PATH_BERT_TITLE, PATH_BERT_TEXT)\n",
    "model.train()\n",
    "train_loss_set = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for part_num in tqdm(range(2,11)):\n",
    "        print(\"Loading data...\")\n",
    "        params = read_param_data(part_num)\n",
    "        params['title'] = prepare_bert_data(params['title'])\n",
    "        x_text, xid_text, at_text = prepare_text_feature(params['text'])\n",
    "        del params['text']\n",
    "\n",
    "        features = prepare_non_text_features(params)\n",
    "        features = torch.tensor(features)\n",
    "        print('Non-text features shape: ', features.shape)\n",
    "        \n",
    "        y = encode_y(params['y'], ohe)\n",
    "        y = torch.tensor(y)\n",
    "        \n",
    "        # Надо стакнуть тензоры так чтобы получилось новое пространство\n",
    "        dataset = TensorDataset(features, \n",
    "                                params['title']['x'], params['title']['attention'], \n",
    "                                x_text, at_text, y)\n",
    "        del y, params\n",
    "        train_loader, test_loader = split(dataset, 0.1)\n",
    "        \n",
    "        # Запуск обучения\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            bert_title, bert_text = {}, {}\n",
    "            features, bert_title['x'], bert_title['attention'], bert_text_x, bert_text_at, labels = batch\n",
    "            features = features.type(torch.FloatTensor)       \n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            bert_title['x'] = bert_title['x'].type(torch.LongTensor)\n",
    "            bert_title['x'] = bert_title['x'].to(device)\n",
    "            bert_title['attention'] = bert_title['attention'].type(torch.LongTensor)\n",
    "            bert_title['attention'] = bert_title['attention'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Длина текстов часто превышает 512 токенов, поэтому я делаю отдельный расчет по частям и потом усредняю выход\n",
    "            for i in range(1): #bert_text_x.shape[2]\n",
    "                tokens = bert_text_x[:, :, i]\n",
    "                if tokens.sum() == 0:\n",
    "                    pass\n",
    "                tokens = tokens.type(torch.LongTensor)\n",
    "                tokens = tokens.to(device)\n",
    "                attention = bert_text_at[:, :, i]\n",
    "                attention = attention.type(torch.LongTensor).to(device)\n",
    "                # вклад текущего текста (более короткие тексты меньше приносят вклад)\n",
    "                token_count = 1 #len(tokens)/512\n",
    "                #for token in tokens:\n",
    "                #    if token > 0:\n",
    "                #        token_count += 1\n",
    "                #token_count = len(tokens)/token_count \n",
    "                if i == 0:\n",
    "                    output = model(features, bert_title, {'x': tokens, 'x_id': None, 'attention': attention})\n",
    "                    predict = output\n",
    "                else:\n",
    "                    # работает очень медленно, если обучать каждый раз\n",
    "                    with torch.no_grad():\n",
    "                        output = model(features, bert_title, {'x': tokens, 'x_id': None, 'attention': attention})\n",
    "                    predict = torch.add(predict, output, alpha=token_count)\n",
    "            #output = torch.div(predict, bert_text_x.shape[2])\n",
    "            output = predict\n",
    "            \n",
    "            # Backward pass\n",
    "            loss = floss(output, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Обновляем параметры и делаем шаг используя посчитанные градиенты\n",
    "            scheduler.step(loss)\n",
    "            \n",
    "            # Обновляем loss\n",
    "            train_loss_set.append(float(loss))  \n",
    "\n",
    "            # Рисуем график\n",
    "            if step % 1000 == 0:\n",
    "                clear_output(True)\n",
    "                plt.plot(train_loss_set)\n",
    "                plt.title(\"Training loss part={} EPOCH={} learning rage = {}\".format(part_num, e, optimizer.param_groups[0]['lr']))\n",
    "                print(\"Training loss {} part={} EPOCH={} learning rage = {}\".format(\n",
    "                    train_loss_set[-1], part_num, e, optimizer.param_groups[0]['lr']))\n",
    "                plt.xlabel(\"Batch\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.ylim(0, 0.03)\n",
    "                plt.show()\n",
    "\n",
    "        \n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        model.save_berts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:21:46.314727Z",
     "start_time": "2020-07-23T08:21:46.310708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:02:14.001545Z",
     "start_time": "2020-07-22T11:02:02.411757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), PATH)\n",
    "model.save_berts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ea82c49f8d4d399904ab939ebf7bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "valid_preds, valid_labels = [], []\n",
    "for batch in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        x, labels = batch\n",
    "        x = x.type(torch.FloatTensor)\n",
    "        x = x.to(device)\n",
    "        output = model(x)\n",
    "        \n",
    "        output = output.detach().cpu().numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        valid_preds.extend(output)\n",
    "        valid_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_preds(y, revert_ohe, threshold=0.5, max_class=6):\n",
    "    result = []\n",
    "    for class_ in y:\n",
    "        class_ = sigmoid(class_)\n",
    "        current_class = []\n",
    "        for i in range(max_class):\n",
    "            top = np.argmax(class_)\n",
    "            # Порог либо первый элемент (хотя бы одна специализация есть)\n",
    "            if class_[top] >= threshold or i==0:\n",
    "                current_class.append(revert_ohe[top])\n",
    "            class_ = np.delete(class_, [top])\n",
    "        result.append(current_class)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T14:10:11.471596Z",
     "start_time": "2020-07-29T14:10:11.465611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.51\n"
     ]
    }
   ],
   "source": [
    "_valid_preds = decode_preds(valid_preds, revert_ohe)\n",
    "_valid_labels = decode_labels(valid_labels, revert_ohe)\n",
    "\n",
    "print(\"Score: {0:.2f}\".format(mean_f1score(_valid_labels, _valid_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit\n",
    "## Токензация текстов\n",
    "выполняется один раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T12:50:07.901004Z",
     "start_time": "2020-07-22T11:27:18.084943Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for part_num in range(2,11):\n",
    "    print(f'Часть {part_num}')\n",
    "    params = read_param_data(part_num, False)\n",
    "    \n",
    "    print('БЕРТ по заголовку')\n",
    "    params['title'] = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in params['names']]\n",
    "    del params['names']\n",
    "    params['title'] = [tokenizer.tokenize(sent) for sent in params['title']]\n",
    "    \n",
    "    print('БЕРТ по скиллам')\n",
    "    params['skills'] = [skill_tokenize(sent) for sent in params['skills']]\n",
    "    # объединяем входы\n",
    "    for i in range(len(params['title'])):\n",
    "        params['title'][i].extend(params['skills'][i])\n",
    "    del params['skills']\n",
    "\n",
    "    print('БЕРТ по тексту')\n",
    "    params['text'] = [\"[CLS] \" + cleanhtml(x) + \" [SEP]\" for x in params['text']]\n",
    "    params['text'] = [tokenizer.tokenize(sent) for sent in tqdm(params['text'])]\n",
    "\n",
    "    f = open(f'submit{part_num}.pkl', 'wb')\n",
    "    pickle.dump(params, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск расчета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T20:31:03.983219Z",
     "start_time": "2020-07-22T20:27:29.597661Z"
    }
   },
   "outputs": [],
   "source": [
    "answers = {}\n",
    "model.eval()\n",
    "for part_num in range(1,2):\n",
    "    params = read_param_data(part_num, False)\n",
    "    \n",
    "    params['title'] = prepare_bert_data(params['title'])\n",
    "    \n",
    "    x_text, xid_text, at_text = prepare_text_feature(params['text'])\n",
    "    del params['text']\n",
    "    \n",
    "    features = prepare_non_text_features(params)\n",
    "    features = torch.tensor(features)\n",
    "    print('Non-text features shape: ', features.shape)  \n",
    "\n",
    "    dataset = TensorDataset(features, \n",
    "                            params['title']['x'], params['title']['attention'], \n",
    "                            x_text, at_text, torch.tensor(params['vacancy']))\n",
    "    del params\n",
    "    \n",
    "    train_loader, test_loader = split(dataset, 0.99)\n",
    "    del test_loader\n",
    "    \n",
    "    preds, vacancy = model.get_outputs(train_loader, train=False, validate=False)    \n",
    "    for i, v in enumerate(vacancy):\n",
    "        answers[int(v)] = decode_preds([preds[i]], revert_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T20:26:57.825570Z",
     "start_time": "2020-07-22T20:26:57.804598Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(vacancy):\n",
    "        answers[int(v)] = decode_preds(preds[i],revert_ohe)\n",
    "        \n",
    "preds = []\n",
    "for value in answers.values():\n",
    "    preds.append(decode_preds([value], revert_ohe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_submission = pd.DataFrame([\n",
    "    (vacancy_id, top3_specs_by_employer.get(vacancy_employers[vacancy_id], [top_spec]))\n",
    "    for vacancy_id in test_ids\n",
    "], columns=['vacancy_id', 'specializations'])\n",
    "sample_submission.to_csv('sample_submission.csv.gz', index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.307px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
